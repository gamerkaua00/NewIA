<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IA Local - Projeto Neural</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Usando uma vers√£o espec√≠fica e est√°vel do WebLLM -->
    <script type="module">
        import * as webllm from "https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.46/+esm";

        // Configura√ß√£o EXPL√çCITA do modelo para evitar erro de URL
        // Estamos apontando diretamente para os bin√°rios compilados (WASM)
        const myAppConfig = {
            model_list: [
                {
                    "model_id": "Phi-3-mini-4k-instruct-q4f16_1-MLC",
                    "model_lib_url": "https://raw.githubusercontent.com/mlc-ai/binary-mlc-llm-libs/main/phi-3-mini-4k-instruct-q4f16_1-MLC/Phi-3-mini-4k-instruct-q4f16_1-MLC-webgpu.wasm",
                    "vram_required_MB": 3000,
                    "low_resource_required": true,
                }
            ]
        }

        const selectedModel = "Phi-3-mini-4k-instruct-q4f16_1-MLC"; 
        
        let engine = null;
        let chatHistory = [];

        const $ = (id) => document.getElementById(id);

        async function initChat() {
            $('status-text').innerText = "Inicializando motor gr√°fico (WebGPU)...";
            $('error-log').classList.add('hidden'); // Esconde erros antigos
            
            const initProgressCallback = (report) => {
                $('status-text').innerText = report.text;
                // Barra de progresso baseada no texto
                if(report.text.includes("Fetching param cache")) {
                    // Extraindo porcentagem se dispon√≠vel
                    const match = report.text.match(/(\d+)%/);
                    if (match) $('loading-bar').style.width = match[1] + "%";
                } else if (report.text.includes("Loading model")) {
                     $('loading-bar').style.width = "90%";
                }
            };

            try {
                // Verifica√ß√£o de suporte WebGPU antes de tentar carregar
                if (!navigator.gpu) {
                    throw new Error("Seu navegador n√£o suporta WebGPU. Tente usar Chrome ou Edge atualizados no Desktop ou Android.");
                }

                // Passamos o appConfig para garantir que ele ache os arquivos
                engine = await webllm.CreateMLCEngine(
                    selectedModel,
                    { 
                        appConfig: myAppConfig,
                        initProgressCallback: initProgressCallback 
                    }
                );
                
                $('status-text').innerText = "IA Pronta!";
                $('loading-bar').style.width = "100%";
                setTimeout(() => $('loading-container').style.display = 'none', 800);
                $('send-btn').disabled = false;
                
                const systemPrompt = `
Voc√™ √© uma IA assistente rodando localmente.
Responda sempre em Portugu√™s do Brasil.
Seja direta e √∫til.

FERRAMENTAS:
Se precisar pesquisar: [BUSCAR: termo]
Se precisar salvar: [SALVAR: dados]
`;
                chatHistory.push({ role: "system", content: systemPrompt });

            } catch (err) {
                $('loading-container').style.display = 'flex'; // Mant√©m vis√≠vel em caso de erro
                $('status-text').innerText = "Falha na inicializa√ß√£o.";
                $('error-log').innerText = err.message;
                $('error-log').classList.remove('hidden');
                console.error("Erro detalhado:", err);
            }
        }

        function appendMessage(role, text) {
            const chatBox = $('chat-box');
            const div = document.createElement('div');
            div.className = `p-3 rounded-lg max-w-[85%] mb-2 text-sm leading-relaxed shadow-sm ${
                role === 'user' 
                ? 'bg-blue-600 text-white self-end ml-auto rounded-tr-none' 
                : 'bg-gray-700 text-gray-200 self-start rounded-tl-none'
            }`;
            
            // Formata√ß√£o simples de Markdown
            let formattedText = text
                .replace(/\*\*(.*?)\*\*/g, '<b class="text-white">$1</b>') // Negrito
                .replace(/### (.*?)\n/g, '<h3 class="text-lg font-bold mt-2">$1</h3>') // T√≠tulos
                .replace(/`(.*?)`/g, '<code class="bg-black/30 px-1 py-0.5 rounded text-yellow-200 font-mono text-xs">$1</code>'); // C√≥digo inline

            div.innerHTML = formattedText;
            chatBox.appendChild(div);
            chatBox.scrollTop = chatBox.scrollHeight;
        }

        window.sendMessage = async () => {
            const input = $('user-input');
            const text = input.value.trim();
            if (!text || !engine) return;

            appendMessage('user', text);
            input.value = '';
            chatHistory.push({ role: "user", content: text });

            const thinkingDiv = document.createElement('div');
            thinkingDiv.id = 'thinking';
            thinkingDiv.className = 'flex items-center gap-2 text-xs text-gray-400 italic ml-2 mb-2';
            thinkingDiv.innerHTML = '<span class="w-2 h-2 bg-blue-500 rounded-full animate-bounce"></span> Processando localmente...';
            $('chat-box').appendChild(thinkingDiv);

            try {
                const reply = await engine.chat.completions.create({
                    messages: chatHistory,
                    temperature: 0.6,
                    max_tokens: 1024,
                });

                const aiResponse = reply.choices[0].message.content;
                $('thinking').remove();

                // L√≥gica de Ferramentas (Simulada para teste)
                if (aiResponse.includes("[BUSCAR:")) {
                    handleSearchTool(aiResponse);
                } else if (aiResponse.includes("[SALVAR:")) {
                    handleSaveTool(aiResponse);
                } else {
                    appendMessage('assistant', aiResponse);
                    chatHistory.push({ role: "assistant", content: aiResponse });
                }

            } catch (err) {
                if($('thinking')) $('thinking').remove();
                appendMessage('assistant', "Erro: " + err.message);
            }
        };

        // Simula√ß√£o das ferramentas (Para testar o "C√©rebro" antes do Backend)
        async function handleSearchTool(command) {
            const query = command.match(/\[BUSCAR: (.*?)\]/)[1];
            appendMessage('assistant', `üîé <i>(Modo Offline) Detectei necessidade de busca: "${query}". Quando conectarmos a API, trarei a resposta real.</i>`);
            
            // Simulando retorno para a IA continuar a conversa
            chatHistory.push({ role: "system", content: `[SISTEMA]: A busca por "${query}" ainda n√£o est√° conectada. Diga ao usu√°rio que voc√™ entendeu a inten√ß√£o mas precisa do m√≥dulo de internet ativado.` });
            
            const followUp = await engine.chat.completions.create({ messages: chatHistory });
            appendMessage('assistant', followUp.choices[0].message.content);
        }

        async function handleSaveTool(command) {
            const data = command.match(/\[SALVAR: (.*?)\]/)[1];
            // Salvando no LocalStorage do navegador (tempor√°rio, para teste)
            const savedItems = JSON.parse(localStorage.getItem('ia_memoria') || '[]');
            savedItems.push({ data: new Date().toISOString(), content: data });
            localStorage.setItem('ia_memoria', JSON.stringify(savedItems));

            appendMessage('assistant', `üíæ <i>Informa√ß√£o salva na mem√≥ria local do navegador: "${data}"</i>`);
            chatHistory.push({ role: "assistant", content: "Item salvo com sucesso." });
        }

        $('user-input').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') window.sendMessage();
        });

        window.onload = initChat;
    </script>
</head>
<body class="bg-gray-900 text-gray-100 h-screen flex flex-col font-sans overflow-hidden">

    <!-- Header -->
    <header class="bg-gray-800 p-4 border-b border-gray-700 flex justify-between items-center shadow-lg z-10">
        <div class="flex items-center gap-3">
            <div class="relative">
                <div class="w-3 h-3 rounded-full bg-green-500 animate-pulse"></div>
                <div class="absolute inset-0 w-3 h-3 rounded-full bg-green-500 blur-sm animate-pulse"></div>
            </div>
            <div>
                <h1 class="font-bold text-lg tracking-wide leading-none">Projeto Neural</h1>
                <span class="text-[10px] text-gray-400 font-mono">MODELO: PHI-3 MINI 4K (Local)</span>
            </div>
        </div>
        
        <!-- Menu de Status -->
        <div class="flex gap-2 text-xs">
            <div class="px-2 py-1 bg-gray-700 rounded border border-gray-600">WebGPU: <span class="text-green-400">Ativo</span></div>
        </div>
    </header>

    <!-- Overlay de Carregamento -->
    <div id="loading-container" class="absolute inset-0 bg-gray-900 z-50 flex flex-col items-center justify-center p-6 text-center">
        <div class="w-full max-w-md bg-gray-800 rounded-xl p-8 shadow-2xl border border-gray-700">
            <h2 class="text-xl font-bold mb-4 text-white">Carregando Modelo Neural</h2>
            
            <div class="w-full bg-gray-900 h-4 rounded-full overflow-hidden mb-2 border border-gray-600">
                <div id="loading-bar" class="h-full bg-blue-600 transition-all duration-300 relative" style="width: 0%">
                    <div class="absolute inset-0 bg-white/20 animate-[pulse_2s_infinite]"></div>
                </div>
            </div>
            
            <p id="status-text" class="text-blue-300 text-sm font-mono mt-2">Iniciando download...</p>
            
            <!-- √Årea de Erro (Oculta por padr√£o) -->
            <div id="error-log" class="hidden mt-4 p-3 bg-red-900/50 border border-red-500 rounded text-red-200 text-xs font-mono text-left overflow-auto max-h-32"></div>

            <p class="text-xs text-gray-500 mt-6 border-t border-gray-700 pt-4">
                ‚ö†Ô∏è Este processo baixa ~2.3GB de dados do modelo Phi-3. <br>
                Isso √© feito apenas uma vez. Nas pr√≥ximas, o carregamento √© instant√¢neo.
            </p>
        </div>
    </div>

    <!-- √Årea do Chat -->
    <main class="flex-1 overflow-y-auto p-4 space-y-4 scroll-smooth bg-gradient-to-b from-gray-900 to-gray-800" id="chat-box">
        <!-- Mensagem Inicial -->
        <div class="p-4 rounded-xl bg-gray-800/80 backdrop-blur border border-gray-700 text-gray-200 self-start max-w-[85%] shadow-lg">
            <p class="mb-2">‚ö° <b>Sistema Iniciado.</b></p>
            <p class="text-sm opacity-80">
                Sou uma IA totalmente offline rodando no seu hardware via WebGPU. 
                Nenhum dado sai deste dispositivo a menos que voc√™ use ferramentas externas.
            </p>
        </div>
    </main>

    <!-- Input -->
    <footer class="bg-gray-800 p-4 border-t border-gray-700 z-10">
        <div class="max-w-4xl mx-auto flex gap-3 relative">
            <input type="text" id="user-input" 
                class="w-full bg-gray-900 border border-gray-600 rounded-xl px-5 py-3 text-white placeholder-gray-500 focus:outline-none focus:border-blue-500 focus:ring-1 focus:ring-blue-500 transition-all shadow-inner"
                placeholder="Converse com a IA..." autocomplete="off">
                
            <button onclick="window.sendMessage()" id="send-btn" disabled
                class="bg-blue-600 hover:bg-blue-500 disabled:bg-gray-700 disabled:text-gray-500 disabled:cursor-not-allowed text-white px-6 rounded-xl font-bold transition-all shadow-lg hover:shadow-blue-500/20 active:scale-95 flex items-center justify-center min-w-[60px]">
                ‚û§
            </button>
        </div>
    </footer>

</body>
</html>
